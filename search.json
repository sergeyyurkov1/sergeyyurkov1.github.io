[
  {
    "objectID": "projects1/welcome/index.html",
    "href": "projects1/welcome/index.html",
    "title": "Project 1",
    "section": "",
    "text": "Project 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yurkov Sergey (谢尔盖)",
    "section": "",
    "text": "Computational social scientist, data analyst, and developer. PhD student. I work with Python, Huawei Cangjie, SQL, HTML/CSS & JavaScript.\n\n⭐⭐⭐⭐⭐ “absolutely professional work big thanks again for dedicated support” – arbeitauslagern, Germany\n⭐⭐⭐⭐⭐ “Professional work big thanks for your support. The result is even better than expected. He really knows what he is doing.” – arbeitauslagern, Germany"
  },
  {
    "objectID": "projects1.html",
    "href": "projects1.html",
    "title": "Projects",
    "section": "",
    "text": "Sea Level Predictor\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMedical Data Visualizer\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSpam Classifier Model\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPage View Time Series Visualizer\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Costs Predictor Model\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nToxicity Classifier in Russian\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBook Recommendation Model\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "draft: true\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nYurkov Sergey\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/FCC-book-recommendation.html",
    "href": "projects/FCC-book-recommendation.html",
    "title": "Book Recommendation Model",
    "section": "",
    "text": "import pandas as pd\nfrom scipy.sparse import csr_matrix\nfrom sklearn.neighbors import NearestNeighbors\nimport requests\n\n\nDownloading the dataset\n\nurl = \"https://cdn.freecodecamp.org/project-data/books/book-crossings.zip\"\nquery_parameters = {\"downloadformat\": \"zip\"}\n\nresponse = requests.get(url, params=query_parameters)\n\nwith open(\"book-crossings.zip\", mode=\"wb\") as file:\n    file.write(response.content)\n\n\nimport zipfile\n\nwith zipfile.ZipFile(\"book-crossings.zip\", mode=\"r\") as archive:\n    for file in archive.namelist():\n        archive.extractall(\"book-crossings/\")\n\n\nbooks_filename = \"book-crossings/BX-Books.csv\"\nratings_filename = \"book-crossings/BX-Book-Ratings.csv\"\n\n\ndf_books = pd.read_csv(\n    books_filename,\n    encoding=\"ISO-8859-1\",\n    sep=\";\",\n    header=0,\n    names=[\"isbn\", \"title\", \"author\"],\n    usecols=[\"isbn\", \"title\", \"author\"],\n    dtype={\"isbn\": \"str\", \"title\": \"str\", \"author\": \"str\"},\n)\n\ndf_ratings = pd.read_csv(\n    ratings_filename,\n    encoding=\"ISO-8859-1\",\n    sep=\";\",\n    header=0,\n    names=[\"user\", \"isbn\", \"rating\"],\n    usecols=[\"user\", \"isbn\", \"rating\"],\n    dtype={\"user\": \"int32\", \"isbn\": \"str\", \"rating\": \"float32\"},\n)\n\n\n\nExploring the dataset\n\ndf_books\n\n\n\n\n\n\n\n\nisbn\ntitle\nauthor\n\n\n\n\n0\n0195153448\nClassical Mythology\nMark P. O. Morford\n\n\n1\n0002005018\nClara Callan\nRichard Bruce Wright\n\n\n2\n0060973129\nDecision in Normandy\nCarlo D'Este\n\n\n3\n0374157065\nFlu: The Story of the Great Influenza Pandemic...\nGina Bari Kolata\n\n\n4\n0393045218\nThe Mummies of Urumchi\nE. J. W. Barber\n\n\n...\n...\n...\n...\n\n\n271374\n0440400988\nThere's a Bat in Bunk Five\nPaula Danziger\n\n\n271375\n0525447644\nFrom One to One Hundred\nTeri Sloat\n\n\n271376\n006008667X\nLily Dale : The True Story of the Town that Ta...\nChristine Wicker\n\n\n271377\n0192126040\nRepublic (World's Classics)\nPlato\n\n\n271378\n0767409752\nA Guided Tour of Rene Descartes' Meditations o...\nChristopher Biffle\n\n\n\n\n271379 rows × 3 columns\n\n\n\n\ndf_ratings\n\n\n\n\n\n\n\n\nuser\nisbn\nrating\n\n\n\n\n0\n276725\n034545104X\n0.0\n\n\n1\n276726\n0155061224\n5.0\n\n\n2\n276727\n0446520802\n0.0\n\n\n3\n276729\n052165615X\n3.0\n\n\n4\n276729\n0521795028\n6.0\n\n\n...\n...\n...\n...\n\n\n1149775\n276704\n1563526298\n9.0\n\n\n1149776\n276706\n0679447156\n0.0\n\n\n1149777\n276709\n0515107662\n10.0\n\n\n1149778\n276721\n0590442449\n10.0\n\n\n1149779\n276723\n05162443314\n8.0\n\n\n\n\n1149780 rows × 3 columns\n\n\n\n\n# Remove from the dataset users with less than 200 ratings and books with less than 100 ratings\n\n# Function adapted from\n# https://datascienceplus.com/building-a-book-recommender-system-the-basics-knn-and-matrix-factorization/\ncounts_user = df_ratings[\"user\"].value_counts()\ncounts_isbn = df_ratings[\"isbn\"].value_counts()\ncounts_user = counts_user[counts_user &gt;= 200].index\ncounts_isbn = counts_isbn[counts_isbn &gt;= 100].index\ndf_ratings_filtered = df_ratings.loc[\n    (df_ratings[\"user\"].isin(counts_user.values))\n    & (df_ratings[\"isbn\"].isin(counts_isbn.values))\n]\n\ndf_ratings_filtered.shape\n\n(49781, 3)\n\n\n\n\nJoining the two dataframes\n\ndf_books_ratings = pd.merge(df_books, df_ratings_filtered, on=\"isbn\")\n\ndf_books_ratings[\"author\"] = df_books_ratings[\"author\"].str.title()\n\ndf_books_ratings\n\n\n\n\n\n\n\n\nisbn\ntitle\nauthor\nuser\nrating\n\n\n\n\n0\n0440234743\nThe Testament\nJohn Grisham\n277478\n0.0\n\n\n1\n0440234743\nThe Testament\nJohn Grisham\n2977\n0.0\n\n\n2\n0440234743\nThe Testament\nJohn Grisham\n3363\n0.0\n\n\n3\n0440234743\nThe Testament\nJohn Grisham\n7346\n9.0\n\n\n4\n0440234743\nThe Testament\nJohn Grisham\n9856\n0.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n49512\n0515135739\nEleventh Hour: An FBI Thriller (FBI Thriller (...\nCatherine Coulter\n236283\n0.0\n\n\n49513\n0515135739\nEleventh Hour: An FBI Thriller (FBI Thriller (...\nCatherine Coulter\n251613\n0.0\n\n\n49514\n0515135739\nEleventh Hour: An FBI Thriller (FBI Thriller (...\nCatherine Coulter\n252071\n0.0\n\n\n49515\n0515135739\nEleventh Hour: An FBI Thriller (FBI Thriller (...\nCatherine Coulter\n256407\n0.0\n\n\n49516\n0515135739\nEleventh Hour: An FBI Thriller (FBI Thriller (...\nCatherine Coulter\n262399\n0.0\n\n\n\n\n49517 rows × 5 columns\n\n\n\n\n\nDropping duplicates and making a pivot table for the KNN algorithm\n\ndf_books_ratings.drop_duplicates([\"title\", \"user\"], keep=\"first\", inplace=True)\n\npivot = df_books_ratings.pivot(index=\"title\", columns=\"user\", values=\"rating\").fillna(0)\n\npivot.shape\n\n(673, 888)\n\n\n\npivot\n\n\n\n\n\n\n\nuser\n254\n2276\n2766\n2977\n3363\n4017\n4385\n6242\n6251\n6323\n...\n274004\n274061\n274301\n274308\n274808\n275970\n277427\n277478\n277639\n278418\n\n\ntitle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1984\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1st to Die: A Novel\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2nd Chance\n0.0\n10.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4 Blondes\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nA Beautiful Mind: The Life of Mathematical Genius and Nobel Laureate John Nash\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nWithout Remorse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nYear of Wonders\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nYou Belong To Me\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nZen and the Art of Motorcycle Maintenance: An Inquiry into Values\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\\O\\\" Is for Outlaw\"\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n673 rows × 888 columns\n\n\n\n\n\nConverting the pivot table above to a sparse matrix that is efficient for arithmetic operations, row slicing, and matrix-vector products. It is particularly useful when dealing with large matrices where most elements are zero, as it saves memory by only storing non-zero elements.\n\nmatrix = csr_matrix(pivot.values)\n\nmatrix\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float32'\n    with 12423 stored elements and shape (673, 888)&gt;\n\n\n\nmodel = NearestNeighbors(algorithm=\"brute\", metric=\"cosine\")\n\nmodel.fit(matrix)\n\nNearestNeighbors(algorithm='brute', metric='cosine')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.NearestNeighbors?Documentation for NearestNeighborsiFittedNearestNeighbors(algorithm='brute', metric='cosine') \n\n\n\n\nGetting the results\n\n# Function to return recommended books\n# adapted from\n# https://datascienceplus.com/building-a-book-recommender-system-the-basics-knn-and-matrix-factorization/\ndef get_recommends(book=\"\"):\n    distances, indices = model.kneighbors(\n        pivot.loc[book, :].values.reshape(1, -1), n_neighbors=5\n    )\n    recommended_books = []\n    recommended_books.append(book)\n    recommended_books.append([])\n    for i in range(1, len(distances.flatten())):\n        recommended_books[1].insert(\n            0, [pivot.index[indices.flatten()[i]], distances.flatten()[i]]\n        )\n\n    return recommended_books\n\n\nimport pprint\n\nrecommends = get_recommends(\"Where the Heart Is (Oprah's Book Club (Paperback))\")\n\npprint.pprint(recommends)\n\n[\"Where the Heart Is (Oprah's Book Club (Paperback))\",\n [['The Weight of Water', np.float32(0.77085835)],\n  ['The Surgeon', np.float32(0.7699411)],\n  ['I Know This Much Is True', np.float32(0.7677075)],\n  ['The Lovely Bones: A Novel', np.float32(0.7234864)]]]\n\n\n\n\nTesting the results\n\ndef test_book_recommendation():\n    test_pass = True\n\n    if recommends[0] != \"Where the Heart Is (Oprah's Book Club (Paperback))\":\n        test_pass = False\n\n    recommended_books = [\n        \"I'll Be Seeing You\",\n        \"The Weight of Water\",\n        \"The Surgeon\",\n        \"I Know This Much Is True\",\n    ]\n    recommended_books_dist = [0.8, 0.77, 0.77, 0.77]\n\n    for i in range(2):\n        if recommends[1][i][0] not in recommended_books:\n            test_pass = False\n        if abs(recommends[1][i][1] - recommended_books_dist[i]) &gt;= 0.05:\n            test_pass = False\n    if test_pass:\n        print(\"You passed the challenge! 🎉🎉🎉🎉🎉\")\n    else:\n        print(\"You haven't passed yet. Keep trying!\")\n\n\ntest_book_recommendation()\n\nYou passed the challenge! 🎉🎉🎉🎉🎉"
  },
  {
    "objectID": "projects/FCC-health-costs-predictor.html",
    "href": "projects/FCC-health-costs-predictor.html",
    "title": "Health Costs Predictor Model",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\n\n# 2.x\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nimport requests\n\nurl = \"https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\"\nquery_parameters = {\"downloadformat\": \"csv\"}\n\nresponse = requests.get(url, params=query_parameters)\n\nwith open(\"insurance.csv\", mode=\"wb\") as file:\n    file.write(response.content)\n\n\ndataset = pd.read_csv(\"insurance.csv\")\ndataset.tail()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\nexpenses\n\n\n\n\n1333\n50\nmale\n31.0\n3\nno\nnorthwest\n10600.55\n\n\n1334\n18\nfemale\n31.9\n0\nno\nnortheast\n2205.98\n\n\n1335\n18\nfemale\n36.9\n0\nno\nsoutheast\n1629.83\n\n\n1336\n21\nfemale\n25.8\n0\nno\nsouthwest\n2007.95\n\n\n1337\n61\nfemale\n29.1\n0\nyes\nnorthwest\n29141.36\n\n\n\n\n\n\n\n\ndataset.isna().sum()\n\n# region_list = dataset[\"region\"].unique()\n# region_map = {i: e for e, i in enumerate(region_list)}\n# region_map\n\nage         0\nsex         0\nbmi         0\nchildren    0\nsmoker      0\nregion      0\nexpenses    0\ndtype: int64\n\n\n\n# dataset[\"sex\"] = dataset[\"sex\"].map({\"male\": 1, \"female\": 2})\n# dataset[\"smoker\"] = dataset[\"smoker\"].map({\"yes\": 1, \"no\": 0})\n# dataset[\"region\"] = dataset[\"sex\"].map(region_map)\n\ndataset = pd.get_dummies(dataset, columns=[\"sex\"], prefix=\"sex\", prefix_sep=\"_\")\ndataset = pd.get_dummies(dataset, columns=[\"smoker\"], prefix=\"smoker\", prefix_sep=\"_\")\ndataset = pd.get_dummies(dataset, columns=[\"region\"], prefix=\"region\", prefix_sep=\"_\")\n\ndataset.tail()\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nexpenses\nsex_female\nsex_male\nsmoker_no\nsmoker_yes\nregion_northeast\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n1333\n50\n31.0\n3\n10600.55\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1334\n18\n31.9\n0\n2205.98\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1335\n18\n36.9\n0\n1629.83\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1336\n21\n25.8\n0\n2007.95\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1337\n61\n29.1\n0\n29141.36\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=0)\n\n\ntrain_features = train_dataset.copy()\ntest_features = test_dataset.copy()\n\ntrain_labels = train_features.pop(\"expenses\")\ntest_labels = test_features.pop(\"expenses\")\n\n\nmodel = keras.Sequential(\n    [\n        layers.Dense(units=16, activation=\"relu\"),\n        layers.Dense(units=8, activation=\"relu\"),\n        layers.Dense(units=1, activation=\"linear\"),\n    ]\n)\n\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                   │ ?                      │   0 (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ ?                      │   0 (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ ?                      │   0 (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 0 (0.00 B)\n\n\n\n Trainable params: 0 (0.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.1),\n    loss=\"mean_absolute_error\",\n    metrics=[\"mae\"],\n)\n\n\nhistory = model.fit(\n    train_features,\n    train_labels,\n    epochs=50,\n    verbose=1,\n    validation_split=0.2,\n)\n\nEpoch 1/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - loss: 12400.3613 - mae: 12400.3613 - val_loss: 8794.2754 - val_mae: 8794.2754\nEpoch 2/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 7730.8154 - mae: 7730.8154 - val_loss: 8197.8213 - val_mae: 8197.8213\nEpoch 3/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6926.5454 - mae: 6926.5454 - val_loss: 7878.8511 - val_mae: 7878.8511\nEpoch 4/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6531.6499 - mae: 6531.6499 - val_loss: 7495.9204 - val_mae: 7495.9204\nEpoch 5/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5858.8516 - mae: 5858.8516 - val_loss: 7229.2598 - val_mae: 7229.2598\nEpoch 6/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 6184.5981 - mae: 6184.5981 - val_loss: 7193.7910 - val_mae: 7193.7910\nEpoch 7/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6406.2319 - mae: 6406.2319 - val_loss: 6683.9468 - val_mae: 6683.9468\nEpoch 8/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 4873.6665 - mae: 4873.6665 - val_loss: 6323.3506 - val_mae: 6323.3506\nEpoch 9/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5404.1587 - mae: 5404.1587 - val_loss: 5840.2300 - val_mae: 5840.2300\nEpoch 10/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4306.5776 - mae: 4306.5776 - val_loss: 4339.7681 - val_mae: 4339.7681\nEpoch 11/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3330.8904 - mae: 3330.8904 - val_loss: 4155.7017 - val_mae: 4155.7017\nEpoch 12/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 3358.3098 - mae: 3358.3098 - val_loss: 4099.0825 - val_mae: 4099.0825\nEpoch 13/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3719.3210 - mae: 3719.3210 - val_loss: 4288.0825 - val_mae: 4288.0825\nEpoch 14/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 3741.3633 - mae: 3741.3633 - val_loss: 4175.4727 - val_mae: 4175.4727\nEpoch 15/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 3244.9753 - mae: 3244.9753 - val_loss: 3973.3491 - val_mae: 3973.3491\nEpoch 16/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2959.1372 - mae: 2959.1372 - val_loss: 3972.8040 - val_mae: 3972.8040\nEpoch 17/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 3164.6609 - mae: 3164.6609 - val_loss: 3850.2273 - val_mae: 3850.2273\nEpoch 18/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2713.0781 - mae: 2713.0781 - val_loss: 3795.8066 - val_mae: 3795.8066\nEpoch 19/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2997.2271 - mae: 2997.2271 - val_loss: 3859.2603 - val_mae: 3859.2603\nEpoch 20/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 3085.3821 - mae: 3085.3821 - val_loss: 3639.3989 - val_mae: 3639.3989\nEpoch 21/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2957.4858 - mae: 2957.4858 - val_loss: 3621.8835 - val_mae: 3621.8835\nEpoch 22/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2837.3000 - mae: 2837.3000 - val_loss: 3489.7847 - val_mae: 3489.7847\nEpoch 23/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2905.4102 - mae: 2905.4102 - val_loss: 3787.1067 - val_mae: 3787.1067\nEpoch 24/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3141.4758 - mae: 3141.4758 - val_loss: 3531.4854 - val_mae: 3531.4854\nEpoch 25/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2478.5315 - mae: 2478.5315 - val_loss: 3507.5911 - val_mae: 3507.5911\nEpoch 26/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2816.0601 - mae: 2816.0601 - val_loss: 3532.2092 - val_mae: 3532.2092\nEpoch 27/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2678.4246 - mae: 2678.4246 - val_loss: 3658.4497 - val_mae: 3658.4497\nEpoch 28/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 2833.5803 - mae: 2833.5803 - val_loss: 3488.4082 - val_mae: 3488.4082\nEpoch 29/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2742.7410 - mae: 2742.7410 - val_loss: 3464.1750 - val_mae: 3464.1750\nEpoch 30/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2677.0469 - mae: 2677.0469 - val_loss: 3482.8792 - val_mae: 3482.8792\nEpoch 31/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2569.9336 - mae: 2569.9336 - val_loss: 3353.8123 - val_mae: 3353.8123\nEpoch 32/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2737.5840 - mae: 2737.5840 - val_loss: 3485.8455 - val_mae: 3485.8455\nEpoch 33/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2512.9038 - mae: 2512.9038 - val_loss: 3309.5256 - val_mae: 3309.5256\nEpoch 34/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2351.0886 - mae: 2351.0886 - val_loss: 3229.2351 - val_mae: 3229.2351\nEpoch 35/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2360.1023 - mae: 2360.1023 - val_loss: 3396.7778 - val_mae: 3396.7778\nEpoch 36/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 2430.5044 - mae: 2430.5044 - val_loss: 3259.0562 - val_mae: 3259.0562\nEpoch 37/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2512.6514 - mae: 2512.6514 - val_loss: 3160.6106 - val_mae: 3160.6106\nEpoch 38/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 2388.4092 - mae: 2388.4092 - val_loss: 3180.7351 - val_mae: 3180.7351\nEpoch 39/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2303.3711 - mae: 2303.3711 - val_loss: 3262.7922 - val_mae: 3262.7922\nEpoch 40/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2160.1658 - mae: 2160.1658 - val_loss: 3144.8740 - val_mae: 3144.8740\nEpoch 41/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2304.2559 - mae: 2304.2559 - val_loss: 3094.5076 - val_mae: 3094.5076\nEpoch 42/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2093.5830 - mae: 2093.5830 - val_loss: 3093.9961 - val_mae: 3093.9961\nEpoch 43/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2485.9470 - mae: 2485.9470 - val_loss: 3144.2881 - val_mae: 3144.2881\nEpoch 44/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2476.5474 - mae: 2476.5474 - val_loss: 3024.2004 - val_mae: 3024.2004\nEpoch 45/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2113.4941 - mae: 2113.4941 - val_loss: 3064.6553 - val_mae: 3064.6553\nEpoch 46/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2147.3889 - mae: 2147.3889 - val_loss: 3155.6006 - val_mae: 3155.6006\nEpoch 47/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2598.4773 - mae: 2598.4773 - val_loss: 3005.9568 - val_mae: 3005.9568\nEpoch 48/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2133.7805 - mae: 2133.7805 - val_loss: 2994.0830 - val_mae: 2994.0830\nEpoch 49/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 2203.0266 - mae: 2203.0266 - val_loss: 2930.2429 - val_mae: 2930.2429\nEpoch 50/50\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 2550.6213 - mae: 2550.6213 - val_loss: 3008.1443 - val_mae: 3008.1443\n\n\n\ndef plot_loss(history):\n    plt.plot(history.history[\"loss\"], label=\"loss\")\n    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n    plt.ylim([0, 15_000])\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Error\")\n    plt.legend()\n    plt.grid(True)\n\n\nplot_loss(history)\n\n# Renaming for the test\ntest_dataset = test_features\n\n\n\n\n\n\n\n\n\n# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.\n# Test model by checking how well the model generalizes using the test set.\n# loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\nloss, mae = model.evaluate(test_dataset, test_labels, verbose=2)\n# mae = model.evaluate(test_dataset, test_labels, verbose=2)\n\nprint(\"Testing set Mean Abs Error: {:5.2f} expenses\".format(mae))\n\nif mae &lt; 3500:\n    print(\"You passed the challenge. Great job!\")\nelse:\n    print(\"The Mean Abs Error must be less than 3500. Keep trying.\")\n\n# Plot predictions.\ntest_predictions = model.predict(test_dataset).flatten()\n\na = plt.axes(aspect=\"equal\")\nplt.scatter(test_labels, test_predictions)\nplt.xlabel(\"True values (expenses)\")\nplt.ylabel(\"Predictions (expenses)\")\nlims = [0, 50000]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)\n\n9/9 - 0s - 6ms/step - loss: 2247.7083 - mae: 2247.7083\nTesting set Mean Abs Error: 2247.71 expenses\nYou passed the challenge. Great job!\n9/9 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \n\n\n\n\n\n\n\n\n\n   Created in Deepnote"
  },
  {
    "objectID": "projects/FCC-spam-classifier-2.html",
    "href": "projects/FCC-spam-classifier-2.html",
    "title": "Spam Classifier Model",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(tf.__version__)\n\n2.18.0\n\n\n\n# import requests\n\n# url = \"https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\"\n# query_parameters = {\"downloadformat\": \"tsv\"}\n\n# response = requests.get(url, params=query_parameters)\n\n# with open(\"train-data.tsv\", mode=\"wb\") as file:\n#     file.write(response.content)\n\n# url = \"https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\"\n# query_parameters = {\"downloadformat\": \"tsv\"}\n\n# response = requests.get(url, params=query_parameters)\n\n# with open(\"valid-data.tsv\", mode=\"wb\") as file:\n#     file.write(response.content)\n\n\ntrain_file_path = \"train-data.tsv\"\ntest_file_path = \"valid-data.tsv\"\n\n\nheader_names = [\"label\", \"message\"]\n\ndf_train = pd.read_csv(\n    train_file_path, sep=\"\\t\", encoding=\"utf-8\", header=None, names=header_names\n)\ndf_test = pd.read_csv(\n    test_file_path, sep=\"\\t\", encoding=\"utf-8\", header=None, names=header_names\n)\n\n\ndf_test.sample(5)\n\n\n\n\n\n\n\n\nlabel\nmessage\n\n\n\n\n1349\nham\nyeah sure thing mate haunt got all my stuff so...\n\n\n725\nham\ni just got home babe, are you still awake ?\n\n\n234\nham\nfinally it has happened..! aftr decades..! bee...\n\n\n1191\nham\nit‘s £6 to get in, is that ok?\n\n\n715\nham\nhello. damn this christmas thing. i think i ha...\n\n\n\n\n\n\n\n\nlabel_map = {\"ham\": 0, \"spam\": 1}\n\ndf_train[\"label\"] = df_train[\"label\"].map(label_map)\ndf_test[\"label\"] = df_test[\"label\"].map(label_map)\n\n\nentity_map = {\n    \"&#33;\": \"!\",\n    \"&#34;\": '\"',\n    \"&#35;\": \"#\",\n    \"&#36;\": \"$\",\n    \"&#37;\": \"%\",\n    \"&#38;\": \"&\",\n    \"&#39;\": \"'\",\n    \"&#40;\": \"(\",\n    \"&#41;\": \")\",\n    \"&#42;\": \"*\",\n    \"&#43;\": \"+\",\n    \"&#44;\": \",\",\n    \"&#45;\": \"-\",\n    \"&#46;\": \".\",\n    \"&#47;\": \"/\",\n    \"&#48;\": \"0\",\n    \"&#49;\": \"1\",\n    \"&#50;\": \"2\",\n    \"&#51;\": \"3\",\n    \"&#52;\": \"4\",\n    \"&#53;\": \"5\",\n    \"&#54;\": \"6\",\n    \"&#55;\": \"7\",\n    \"&#56;\": \"8\",\n    \"&#57;\": \"9\",\n    \"&#58;\": \":\",\n    \"&#59;\": \";\",\n    \"&#60;\": \"&lt;\",\n    \"&#61;\": \"=\",\n    \"&#62;\": \"&gt;\",\n    \"&#63;\": \"?\",\n    \"&#64;\": \"@\",\n    \"&#65;\": \"A\",\n    \"&#66;\": \"B\",\n    \"&#67;\": \"C\",\n    \"&#68;\": \"D\",\n    \"&#69;\": \"E\",\n    \"&#70;\": \"F\",\n    \"&#71;\": \"G\",\n    \"&#72;\": \"H\",\n    \"&#73;\": \"I\",\n    \"&#74;\": \"J\",\n    \"&#75;\": \"K\",\n    \"&#76;\": \"L\",\n    \"&#77;\": \"M\",\n    \"&#78;\": \"N\",\n    \"&#79;\": \"O\",\n    \"&#80;\": \"P\",\n    \"&#81;\": \"Q\",\n    \"&#82;\": \"R\",\n    \"&#83;\": \"S\",\n    \"&#84;\": \"T\",\n    \"&#85;\": \"U\",\n    \"&#86;\": \"V\",\n    \"&#87;\": \"W\",\n    \"&#88;\": \"X\",\n    \"&#89;\": \"Y\",\n    \"&#90;\": \"Z\",\n    \"&#91;\": \"[\",\n    \"&#92;\": \"\\\\\",\n    \"&#93;\": \"]\",\n    \"&#94;\": \"^\",\n    \"&#95;\": \"_\",\n    \"&#96;\": \"`\",\n    \"&#97;\": \"a\",\n    \"&#98;\": \"b\",\n    \"&#99;\": \"c\",\n    \"&#100;\": \"d\",\n    \"&#101;\": \"e\",\n    \"&#102;\": \"f\",\n    \"&#103;\": \"g\",\n    \"&#104;\": \"h\",\n    \"&#105;\": \"i\",\n    \"&#106;\": \"j\",\n    \"&#107;\": \"k\",\n    \"&#108;\": \"l\",\n    \"&#109;\": \"m\",\n    \"&#110;\": \"n\",\n    \"&#111;\": \"o\",\n    \"&#112;\": \"p\",\n    \"&#113;\": \"q\",\n    \"&#114;\": \"r\",\n    \"&#115;\": \"s\",\n    \"&#116;\": \"t\",\n    \"&#117;\": \"u\",\n    \"&#118;\": \"v\",\n    \"&#119;\": \"w\",\n    \"&#120;\": \"x\",\n    \"&#121;\": \"y\",\n    \"&#122;\": \"z\",\n    \"&#123;\": \"{\",\n    \"&#124;\": \"|\",\n    \"&#125;\": \"}\",\n    \"&#126;\": \"~\",\n    \"&larr;\": \"←\",\n    \"&uarr;\": \"↑\",\n    \"&rarr;\": \"→\",\n    \"&darr;\": \"↓\",\n    \"&harr;\": \"↔\",\n    \"&lArr;\": \"⇐\",\n    \"&uArr;\": \"⇑\",\n    \"&rArr;\": \"⇒\",\n    \"&dArr;\": \"⇓\",\n    \"&hArr;\": \"⇔\",\n    \"&lsquo;\": \"‘\",\n    \"&rsquo;\": \"’\",\n    \"&ldquo;\": \"“\",\n    \"&rdquo;\": \"”\",\n    \"&#8218;\": \"‚\",\n    \"&#8222;\": \"„\",\n    \"&ndash;\": \"-\",\n    \"&mdash;\": \"–\",\n    \"&nbsp;\": \" \",\n    \"&iexcl;\": \"¡\",\n    \"&sect;\": \"§\",\n    \"&brvbar;\": \"¦\",\n    \"&copy;\": \"©\",\n    \"&reg;\": \"®\",\n    \"&#8482;\": \"™\",\n    \"&cent;\": \"¢\",\n    \"&pound;\": \"£\",\n    \"&yen;\": \"¥\",\n    \"&euro;\": \"€\",\n    \"&plusmn;\": \"±\",\n    \"&micro;\": \"µ\",\n    \"&183;\": \"·\",\n    \"&deg;\": \"°\",\n    \"&sup1;\": \"¹\",\n    \"&sup2;\": \"²\",\n    \"&sup3;\": \"³\",\n    \"&para;\": \"¶\",\n    \"&middot;\": \"·\",\n    \"&frac14;\": \"¼\",\n    \"&frac12;\": \"½\",\n    \"&frac34;\": \"¾\",\n    \"&iquest;\": \"¿\",\n    \"&#8224;\": \"†\",\n    \"&#8225;\": \"‡\",\n    \"&#8226;\": \"•\",\n    \"&#8230;\": \"…\",\n}\n\n\ntrain_X = df_train[\"message\"]\ntest_X = df_test[\"message\"]\n\ntrain_y = df_train[\"label\"]\ntest_y = df_test[\"label\"]\n\n\ntrain_X\n\n0       ahhhh...just woken up!had a bad dream about u ...\n1                                you can never do nothing\n2       now u sound like manky scouse boy steve,like! ...\n3       mum say we wan to go then go... then she can s...\n4       never y lei... i v lazy... got wat? dat day ü ...\n                              ...                        \n4174    just woke up. yeesh its late. but i didn't fal...\n4175    what do u reckon as need 2 arrange transport i...\n4176    free entry into our £250 weekly competition ju...\n4177    -pls stop bootydelious (32/f) is inviting you ...\n4178    tell my  bad character which u dnt lik in me. ...\nName: message, Length: 4179, dtype: object\n\n\n\nimport re\nimport string\n\n\ndef custom_standardizer(x):\n    for i in entity_map:\n        x = tf.strings.regex_replace(x, re.escape(i), entity_map.get(i, \"\"))\n    x = tf.strings.lower(x)\n    x = tf.strings.regex_replace(x, f\"[{re.escape(string.punctuation)}]\", \"\")\n\n    return x\n\n\nvectorization = keras.layers.TextVectorization(\n    # standardize=custom_standardizer,\n    max_tokens=20_000,\n    output_mode=\"int\",\n    output_sequence_length=500,\n)\n\nvectorization.adapt(np.array(train_X))\n\n\nmodel = tf.keras.Sequential(\n    [\n        vectorization,\n        keras.layers.Embedding(20_000 + 1, 128, input_length=500),\n        keras.layers.Dropout(0.5),\n        keras.layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),\n        keras.layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),\n        keras.layers.GlobalMaxPooling1D(),\n        keras.layers.Dense(128, activation=\"relu\"),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(1, activation=\"sigmoid\", name=\"predictions\"),\n    ]\n)\n\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"],\n)\n\nmodel.summary()\n\nd:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n\n\nModel: \"sequential_2\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ text_vectorization_1            │ ?                      │   0 (unbuilt) │\n│ (TextVectorization)             │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding_2 (Embedding)         │ ?                      │   0 (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (Dropout)             │ ?                      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_4 (Conv1D)               │ ?                      │   0 (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_5 (Conv1D)               │ ?                      │   0 (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_max_pooling1d_2          │ ?                      │             0 │\n│ (GlobalMaxPooling1D)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ ?                      │   0 (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (Dropout)             │ ?                      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ predictions (Dense)             │ ?                      │   0 (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 0 (0.00 B)\n\n\n\n Trainable params: 0 (0.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nhistory = model.fit(\n    train_X,\n    train_y,\n    validation_data=(test_X, test_y),\n    batch_size=32,\n    epochs=3,\n    verbose=1,\n)\n\nEpoch 1/3\n131/131 ━━━━━━━━━━━━━━━━━━━━ 8s 43ms/step - accuracy: 0.8469 - loss: 0.4181 - val_accuracy: 0.9820 - val_loss: 0.0693\nEpoch 2/3\n131/131 ━━━━━━━━━━━━━━━━━━━━ 5s 40ms/step - accuracy: 0.9828 - loss: 0.0651 - val_accuracy: 0.9856 - val_loss: 0.0606\nEpoch 3/3\n131/131 ━━━━━━━━━━━━━━━━━━━━ 5s 39ms/step - accuracy: 0.9955 - loss: 0.0202 - val_accuracy: 0.9820 - val_loss: 0.0609\n\n\n\ndef plot_loss(history):\n    plt.plot(history.history[\"loss\"], label=\"tra_loss\")\n    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n    plt.ylim([0, 0.5])\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Error\")\n    plt.legend()\n    plt.grid(True)\n\n\nplot_loss(history)\n\n\n\n\n\n\n\n\n\n# function to predict messages based on model\n# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\ndef predict_message(pred_text):\n    if type(pred_text) is str:\n        pred_text = np.array([pred_text]).astype(object)\n    if type(pred_text) is list:\n        pred_text = np.array(pred_text).astype(object)\n\n    prediction = model.predict(pred_text)\n\n    score = prediction[0][0]\n    if score &gt; 0.5:\n        return [score, \"spam\"]\n    else:\n        return [score, \"ham\"]\n\n\n# pred_text = \"how are you doing today?\"\npred_text = [\n    \"how are you doing today\",\n    \"sale today! to stop texts call 98912460324\",\n    \"i dont want to go. can we try it a different day? available sat\",\n    \"our new mobile video service is live. just install on your phone to start watching.\",\n    \"you have won £1000 cash! call to claim your prize.\",\n    \"i'll bring it tomorrow. don't forget the milk.\",\n    \"wow, is your arm alright. that happened to me one time too\",\n]\n\nprediction = predict_message(pred_text)\nprint(prediction)\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 183ms/step\n[np.float32(0.0054872185), 'ham']\n\n\n\n# Run this cell to test your function and model. Do not modify contents.\ndef test_predictions():\n    test_messages = [\n        \"how are you doing today\",\n        \"sale today! to stop texts call 98912460324\",\n        \"i dont want to go. can we try it a different day? available sat\",\n        \"our new mobile video service is live. just install on your phone to start watching.\",\n        \"you have won £1000 cash! call to claim your prize.\",\n        \"i'll bring it tomorrow. don't forget the milk.\",\n        \"wow, is your arm alright. that happened to me one time too\",\n    ]\n\n    test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n    passed = True\n\n    for msg, ans in zip(test_messages, test_answers):\n        prediction = predict_message(msg)\n        if prediction[1] != ans:\n            passed = False\n\n    if passed:\n        print(\"You passed the challenge. Great job!\")\n    else:\n        print(\"You haven't passed yet. Keep trying.\")\n\n\ntest_predictions()\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 197ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 50ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step\nYou passed the challenge. Great job!"
  },
  {
    "objectID": "projects/sea_level_predictor.html",
    "href": "projects/sea_level_predictor.html",
    "title": "Sea Level Predictor",
    "section": "",
    "text": "%pip install scipy pandas matplotlib\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import linregress\n\n\n# Read data from file\ndf = pd.read_csv(\"data/epa-sea-level.csv\")\n\nprint(df.describe())\nprint(df.info())\nprint(df.head())\n\n              Year  CSIRO Adjusted Sea Level  Lower Error Bound  \\\ncount   134.000000                134.000000         134.000000   \nmean   1946.500000                  3.650341           3.204666   \nstd      38.826537                  2.485692           2.663781   \nmin    1880.000000                 -0.440945          -1.346457   \n25%    1913.250000                  1.632874           1.078740   \n50%    1946.500000                  3.312992           2.915354   \n75%    1979.750000                  5.587598           5.329724   \nmax    2013.000000                  9.326772           8.992126   \n\n       Upper Error Bound  NOAA Adjusted Sea Level  \ncount         134.000000                21.000000  \nmean            4.096016                 7.363746  \nstd             2.312581                 0.691038  \nmin             0.464567                 6.297493  \n25%             2.240157                 6.848690  \n50%             3.710630                 7.488353  \n75%             5.845472                 7.907365  \nmax             9.661417                 8.546648  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 134 entries, 0 to 133\nData columns (total 5 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Year                      134 non-null    int64  \n 1   CSIRO Adjusted Sea Level  134 non-null    float64\n 2   Lower Error Bound         134 non-null    float64\n 3   Upper Error Bound         134 non-null    float64\n 4   NOAA Adjusted Sea Level   21 non-null     float64\ndtypes: float64(4), int64(1)\nmemory usage: 5.4 KB\nNone\n   Year  CSIRO Adjusted Sea Level  Lower Error Bound  Upper Error Bound  \\\n0  1880                  0.000000          -0.952756           0.952756   \n1  1881                  0.220472          -0.732283           1.173228   \n2  1882                 -0.440945          -1.346457           0.464567   \n3  1883                 -0.232283          -1.129921           0.665354   \n4  1884                  0.590551          -0.283465           1.464567   \n\n   NOAA Adjusted Sea Level  \n0                      NaN  \n1                      NaN  \n2                      NaN  \n3                      NaN  \n4                      NaN  \n\n\n\ndef draw_plot():\n    x = df[\"Year\"]\n    y = df[\"CSIRO Adjusted Sea Level\"]\n\n    # Create scatter plot\n    plt.scatter(x, y)\n\n    # Create first line of best fit\n    # Adapted from https://stackoverflow.com/questions/61205263/how-can-i-extend-a-linear-regression-line-and-predict-the-future\n    extended_x_1 = [*range(df.iloc[0, 0], 2051, 1)]\n\n    res = linregress(x, y)\n\n    line = [res.slope * x + res.intercept for x in extended_x_1]\n\n    plt.plot(extended_x_1, line)\n\n    # Create second line of best fit\n    x_2000_2050 = df.loc[df[\"Year\"] &gt;= 2000, \"Year\"]\n    y_2000_2050 = df.loc[df[\"Year\"] &gt;= 2000, \"CSIRO Adjusted Sea Level\"]\n\n    extended_x_2 = [*range(2000, 2051, 1)]\n\n    res_2000_2050 = linregress(x_2000_2050, y_2000_2050)\n\n    line_2000_2050 = [\n        res_2000_2050.slope * x + res_2000_2050.intercept for x in extended_x_2\n    ]\n\n    plt.plot(extended_x_2, line_2000_2050)\n\n    # Add labels and title\n    plt.xlabel(\"Year\")\n    plt.ylabel(\"Sea Level (inches)\")\n    plt.title(\"Rise in Sea Level\")\n\n    # plt.savefig(\"sea_level_plot.png\")\n    return plt.gca()\n\n\nplt.show(draw_plot())"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects/medical_data_visualizer.html",
    "href": "projects/medical_data_visualizer.html",
    "title": "Medical Data Visualizer",
    "section": "",
    "text": "Installing the dependencies\n\npip install pandas seaborn\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Import data\ndf = pd.read_csv(\"data/medical_examination.csv\")\n\ndf\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\n\n\n\n\n0\n0\n18393\n2\n168\n62.0\n110\n80\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n20228\n1\n156\n85.0\n140\n90\n3\n1\n0\n0\n1\n1\n\n\n2\n2\n18857\n1\n165\n64.0\n130\n70\n3\n1\n0\n0\n0\n1\n\n\n3\n3\n17623\n2\n169\n82.0\n150\n100\n1\n1\n0\n0\n1\n1\n\n\n4\n4\n17474\n1\n156\n56.0\n100\n60\n1\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n69995\n99993\n19240\n2\n168\n76.0\n120\n80\n1\n1\n1\n0\n1\n0\n\n\n69996\n99995\n22601\n1\n158\n126.0\n140\n90\n2\n2\n0\n0\n1\n1\n\n\n69997\n99996\n19066\n2\n183\n105.0\n180\n90\n3\n1\n0\n1\n0\n1\n\n\n69998\n99998\n22431\n1\n163\n72.0\n135\n80\n1\n2\n0\n0\n0\n1\n\n\n69999\n99999\n20540\n1\n170\n72.0\n120\n80\n2\n1\n0\n0\n1\n0\n\n\n\n\n70000 rows × 13 columns\n\n\n\n\nprint(df.describe())\nprint(df.info())\n\n                 id           age        gender        height        weight  \\\ncount  70000.000000  70000.000000  70000.000000  70000.000000  70000.000000   \nmean   49972.419900  19468.865814      1.349843    164.359229     74.205690   \nstd    28851.302323   2467.251667      0.477253      8.210126     14.395757   \nmin        0.000000  10798.000000      1.000000     55.000000     10.000000   \n25%    25006.750000  17664.000000      1.000000    159.000000     65.000000   \n50%    50001.500000  19703.000000      1.000000    165.000000     72.000000   \n75%    74889.250000  21327.000000      2.000000    170.000000     82.000000   \nmax    99999.000000  23713.000000      3.000000    250.000000    200.000000   \n\n              ap_hi         ap_lo   cholesterol          gluc         smoke  \\\ncount  70000.000000  70000.000000  70000.000000  70000.000000  70000.000000   \nmean     128.817286     96.630414      1.366871      1.226457      0.088129   \nstd      154.011419    188.472530      0.680250      0.572270      0.283484   \nmin     -150.000000    -70.000000      1.000000      1.000000      0.000000   \n25%      120.000000     80.000000      1.000000      1.000000      0.000000   \n50%      120.000000     80.000000      1.000000      1.000000      0.000000   \n75%      140.000000     90.000000      2.000000      1.000000      0.000000   \nmax    16020.000000  11000.000000      3.000000      3.000000      1.000000   \n\n               alco        active        cardio  \ncount  70000.000000  70000.000000  70000.000000  \nmean       0.053771      0.803729      0.499700  \nstd        0.225568      0.397179      0.500003  \nmin        0.000000      0.000000      0.000000  \n25%        0.000000      1.000000      0.000000  \n50%        0.000000      1.000000      0.000000  \n75%        0.000000      1.000000      1.000000  \nmax        1.000000      1.000000      1.000000  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 70000 entries, 0 to 69999\nData columns (total 13 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   id           70000 non-null  int64  \n 1   age          70000 non-null  int64  \n 2   gender       70000 non-null  int64  \n 3   height       70000 non-null  int64  \n 4   weight       70000 non-null  float64\n 5   ap_hi        70000 non-null  int64  \n 6   ap_lo        70000 non-null  int64  \n 7   cholesterol  70000 non-null  int64  \n 8   gluc         70000 non-null  int64  \n 9   smoke        70000 non-null  int64  \n 10  alco         70000 non-null  int64  \n 11  active       70000 non-null  int64  \n 12  cardio       70000 non-null  int64  \ndtypes: float64(1), int64(12)\nmemory usage: 6.9 MB\nNone\n\n\n\n# Add \"overweight\" column\ndf[\"overweight\"] = np.where((df[\"weight\"] / (df[\"height\"] / 100) ** 2) &gt; 25, 1, 0)\n\n# Normalize data by making 0 always good and 1 always bad. If the value of \"cholesterol\" or \"gluc\" is 1, make the value 0. If the value is more than 1, make the value 1.\ndf[\"cholesterol\"] = np.where(df[\"cholesterol\"] == 1, 0, 1)\ndf[\"gluc\"] = np.where(df[\"gluc\"] == 1, 0, 1)\n\ndf\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\noverweight\n\n\n\n\n0\n0\n18393\n2\n168\n62.0\n110\n80\n0\n0\n0\n0\n1\n0\n0\n\n\n1\n1\n20228\n1\n156\n85.0\n140\n90\n1\n0\n0\n0\n1\n1\n1\n\n\n2\n2\n18857\n1\n165\n64.0\n130\n70\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n3\n17623\n2\n169\n82.0\n150\n100\n0\n0\n0\n0\n1\n1\n1\n\n\n4\n4\n17474\n1\n156\n56.0\n100\n60\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n69995\n99993\n19240\n2\n168\n76.0\n120\n80\n0\n0\n1\n0\n1\n0\n1\n\n\n69996\n99995\n22601\n1\n158\n126.0\n140\n90\n1\n1\n0\n0\n1\n1\n1\n\n\n69997\n99996\n19066\n2\n183\n105.0\n180\n90\n1\n0\n0\n1\n0\n1\n1\n\n\n69998\n99998\n22431\n1\n163\n72.0\n135\n80\n0\n1\n0\n0\n0\n1\n1\n\n\n69999\n99999\n20540\n1\n170\n72.0\n120\n80\n1\n0\n0\n0\n1\n0\n0\n\n\n\n\n70000 rows × 14 columns\n\n\n\n\n# Draw Categorical Plot\ndef draw_cat_plot():\n    # Create DataFrame for cat plot using `pd.melt` using just the values from \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\", and \"overweight\".\n    df_cat = pd.melt(\n        df,\n        id_vars=[\"cardio\"],\n        value_vars=[\"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\", \"overweight\"],\n    )\n\n    # Group and reformat the data to split it by \"cardio\". Show the counts of each feature. You will have to rename one of the columns for the catplot to work correctly.\n    # Solution found at https://forum.freecodecamp.org/t/medical-data-visualizer-confusion/410074/44\n    df_cat = (\n        df_cat.groupby([\"cardio\", \"variable\", \"value\"], as_index=False)\n        .size()\n        .rename(columns={\"size\": \"total\"})\n    )\n\n    # Draw the catplot with \"sns.catplot()\"\n    fig = sns.catplot(\n        data=df_cat, x=\"variable\", y=\"total\", col=\"cardio\", hue=\"value\", kind=\"bar\"\n    ).fig\n\n    # fig.savefig(\"catplot.png\")\n    return fig\n\n\nplt.show(draw_cat_plot())\n\n\n\n\n\n\n\n\n\n# Draw Heat Map\ndef draw_heat_map():\n    # Clean the data\n    df_heat = df.loc[\n        (df[\"ap_lo\"] &lt;= df[\"ap_hi\"])\n        & (df[\"height\"] &gt;= df[\"height\"].quantile(0.025))\n        & (df[\"height\"] &lt;= df[\"height\"].quantile(0.975))\n        & (df[\"weight\"] &gt;= df[\"weight\"].quantile(0.025))\n        & (df[\"weight\"] &lt;= df[\"weight\"].quantile(0.975))\n    ]\n\n    # Calculate the correlation matrix\n    corr = df_heat.corr()\n\n    # Generate a mask for the upper triangle\n    # Hint found at https://www.geeksforgeeks.org/how-to-create-a-triangle-correlation-heatmap-in-seaborn-python/\n    mask = np.triu(corr)\n\n    # Set up the matplotlib figure\n    fig, ax = plt.subplots()\n\n    # Draw the heatmap with \"sns.heatmap()\"\n    sns.heatmap(corr, mask=mask, ax=ax, square=True, annot=True, fmt=\".1f\")\n\n    fig.savefig(\"heatmap.png\")\n    return fig\n\n\nplt.show(draw_heat_map())"
  },
  {
    "objectID": "projects/time_series_visualizer.html",
    "href": "projects/time_series_visualizer.html",
    "title": "Page View Time Series Visualizer",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\n\n# Import data (Make sure to parse dates. Consider setting index column to \"date\".)\ndf = pd.read_csv(\"data/fcc-forum-pageviews.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2016-05-09\n1201\n\n\n1\n2016-05-10\n2329\n\n\n2\n2016-05-11\n1716\n\n\n3\n2016-05-12\n10539\n\n\n4\n2016-05-13\n6933\n\n\n\n\n\n\n\n\nprint(df.describe())\nprint(df.info())\n\n\n# Clean data\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\ndf = df.loc[\n    (df[\"value\"] &gt;= df[\"value\"].quantile(0.025))\n    & (df[\"value\"] &lt;= df[\"value\"].quantile(0.975))\n]\n\ndf\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n10\n2016-05-19\n19736\n\n\n17\n2016-05-26\n18060\n\n\n18\n2016-05-27\n19997\n\n\n19\n2016-05-28\n19044\n\n\n20\n2016-05-29\n20325\n\n\n...\n...\n...\n\n\n1294\n2019-11-24\n138875\n\n\n1299\n2019-11-29\n171584\n\n\n1300\n2019-11-30\n141161\n\n\n1301\n2019-12-01\n142918\n\n\n1303\n2019-12-03\n158549\n\n\n\n\n1238 rows × 2 columns\n\n\n\n\ndef draw_line_plot():\n    df_line = df.copy()\n    df_line.reset_index(inplace=True)\n\n    # Draw line plot\n    fig, ax = plt.subplots()\n    ax.plot(df_line[\"date\"], df_line[\"value\"])\n    ax.set_title(\"Daily freeCodeCamp Forum Page Views 5/2016-12/2019\")\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"Page Views\")\n\n    # fig.savefig(\"line_plot.png\")\n    return fig\n\n\nplt.show(draw_line_plot())\n\n\n\n\n\n\n\n\n\ndef draw_bar_plot():\n    # Copy and modify data for monthly bar plot\n    df_bar = df.copy()\n    df_bar.reset_index(inplace=True)\n\n    df_bar[\"year\"] = df_bar[\"date\"].dt.year\n    df_bar[\"month\"] = df_bar[\"date\"].dt.month_name()\n    df_bar[\"day\"] = df_bar[\"date\"].dt.day\n\n    df_bar = df_bar.groupby([\"year\", \"month\"], as_index=False)[\"value\"].mean()\n\n    # Draw bar plot\n    fig, ax = plt.subplots()\n\n    ax = sns.barplot(data=df_bar, x=\"year\", y=\"value\", hue=\"month\")\n\n    ax.set_xlabel(\"Years\")\n    ax.set_ylabel(\"Average Page Views\")\n\n    sorted_labels = [\n        \"January\",\n        \"February\",\n        \"March\",\n        \"April\",\n        \"May\",\n        \"June\",\n        \"July\",\n        \"August\",\n        \"September\",\n        \"October\",\n        \"November\",\n        \"December\",\n    ]\n\n    # This way labels always default to black color\n    # ax.legend(sorted_labels, title=\"Months\", loc=\"upper left\")\n\n    # Workaround for unsorted labels\n    # Hint found at https://www.c-sharpcorner.com/article/a-complete-python-seaborn-tutorial/\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles[:], sorted_labels, title=\"Months\")\n\n    # fig.savefig(\"bar_plot.png\")\n    return fig\n\n\nplt.show(draw_bar_plot())\n\n\n\n\n\n\n\n\n\ndef draw_box_plot():\n    # Prepare data for box plots (this part is done!)\n    df_box = df.copy()\n    df_box.reset_index(inplace=True)\n\n    df_box[\"year\"] = [d.year for d in df_box.date]\n    df_box[\"month\"] = [d.strftime(\"%b\") for d in df_box.date]\n\n    sns.set(rc={\"figure.figsize\": (10, 8)})\n\n    # Draw box plots (using Seaborn)\n    fig, ax = plt.subplots(1, 2)\n\n    sns.boxplot(data=df_box, ax=ax[0], x=\"year\", y=\"value\")\n    ax[0].set_xlabel(\"Year\")\n    ax[0].set_ylabel(\"Page Views\")\n    ax[0].set_title(\"Year-wise Box Plot (Trend)\")\n\n    sns.boxplot(\n        data=df_box,\n        ax=ax[1],\n        x=\"month\",\n        order=[\n            \"Jan\",\n            \"Feb\",\n            \"Mar\",\n            \"Apr\",\n            \"May\",\n            \"Jun\",\n            \"Jul\",\n            \"Aug\",\n            \"Sep\",\n            \"Oct\",\n            \"Nov\",\n            \"Dec\",\n        ],\n        y=\"value\",\n    )\n    ax[1].set_xlabel(\"Month\")\n    ax[1].set_ylabel(\"Page Views\")\n    ax[1].set_title(\"Month-wise Box Plot (Seasonality)\")\n\n    # fig.savefig(\"box_plot.png\")\n    return fig\n\n\nplt.show(draw_box_plot())"
  },
  {
    "objectID": "projects/bLSTM-toxicity-classifier-RU.html",
    "href": "projects/bLSTM-toxicity-classifier-RU.html",
    "title": "Toxicity Classifier in Russian",
    "section": "",
    "text": "%%capture\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(tf.__version__)\n\n\n# !unzip archive.zip -n\n\n# import zipfile\n\n# with zipfile.ZipFile(\"archive.zip\", mode=\"r\") as archive:\n#     for file in archive.namelist():\n#         archive.extractall(\"archive/\")\n\n\ndf = pd.read_csv(\"archive/labeled.csv\", dtype={\"toxic\": np.int8})\n\ndf\n\n\n\n\n\n\n\n\ncomment\ntoxic\n\n\n\n\n0\nВерблюдов-то за что? Дебилы, бл...\\n\n1\n\n\n1\nХохлы, это отдушина затюканого россиянина, мол...\n1\n\n\n2\nСобаке - собачья смерть\\n\n1\n\n\n3\nСтраницу обнови, дебил. Это тоже не оскорблени...\n1\n\n\n4\nтебя не убедил 6-страничный пдф в том, что Скр...\n1\n\n\n...\n...\n...\n\n\n14407\nВонючий совковый скот прибежал и ноет. А вот и...\n1\n\n\n14408\nА кого любить? Гоблина тупорылого что-ли? Или ...\n1\n\n\n14409\nПосмотрел Утомленных солнцем 2. И оказалось, ч...\n0\n\n\n14410\nКРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...\n1\n\n\n14411\nДо сих пор пересматриваю его видео. Орамбо кст...\n0\n\n\n\n\n14412 rows × 2 columns\n\n\n\n\n# Create data sets for defaults and non-defaults\nnondefaults = df[df[\"toxic\"] == 0]\ndefaults = df[df[\"toxic\"] == 1]\n\n# Undersample the non-defaults\nnondefaults_under = nondefaults.sample(len(defaults))\n\n# Concatenate the undersampled nondefaults with defaults\ndf_balanced = pd.concat(\n    [nondefaults_under.reset_index(drop=True), defaults.reset_index(drop=True)], axis=0\n)\n\n# Print the value counts for loan status\nprint(df_balanced[\"toxic\"].value_counts())\n\ntoxic\n0    4826\n1    4826\nName: count, dtype: int64\n\n\n\ndf_balanced.sample(5)\n\n\n\n\n\n\n\n\ncomment\ntoxic\n\n\n\n\n4317\nКакие интересные крайности\\n\n0\n\n\n3333\nА-А-А-А-А-А--А-А!!!!!!!\\n\n1\n\n\n3336\nСтекло разбить. Зная наших мусоров, они даже н...\n1\n\n\n2571\nЭто неизлечимо. К старости плотность нейронов ...\n1\n\n\n2581\nВсе конечно правильно, но... За 2 минуты люди ...\n0\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_balanced[\"comment\"]\ny = df_balanced[\"toxic\"]\n\ny\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    shuffle=False,\n    random_state=0,  # shuffle=True\n)\n\ny_train.value_counts()\n\ntoxic\n0    4826\n1    2895\nName: count, dtype: int64\n\n\n\nfrom keras_preprocessing import text\nfrom keras_preprocessing import sequence\n\n\nimport string\n\n# from keras.preprocessing.sequence import pad_sequences\nfrom natasha import Doc, MorphVocab, NewsEmbedding, NewsMorphTagger, Segmenter\n\nsegmenter = Segmenter()\nmorph_vocab = MorphVocab()\nemb = NewsEmbedding()\nmorph_tagger = NewsMorphTagger(emb)\n\nprint(morph_vocab)\n\nmax_features = 20_000\n\n\nclass RuTokenizer(text.Tokenizer):\n    def __init__(self):\n        super().__init__(self)\n\n        self.num_words = max_features\n\n    def tokenize(self, text):\n        doc = Doc(text)\n        doc.segment(segmenter)\n        doc.tag_morph(morph_tagger)\n        for token in doc.tokens:\n            token.lemmatize(morph_vocab)\n        # tokens = [_.lemma for _ in doc.tokens if _.text not in string.punctuation]\n        tokens = [\n            _.lemma.lower()\n            for _ in doc.tokens\n            if _.text not in string.punctuation\n            and len(_.text) &gt; 1\n            and not _.text.isnumeric()\n        ]\n\n        return tokens\n\n\ntokenizer = RuTokenizer()\n# df_balanced[\"tokens\"] = df_balanced[\"comment\"].apply(\n#     lambda x: repr(tokenizer.tokenize(x))\n# )\n# df_balanced[\"tokens\"]\nmaxlen = 300\n\ntokenizer.fit_on_texts(X_train)\nsequences = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(sequences, maxlen=maxlen)\n\nsequences = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(sequences, maxlen=maxlen)\n\nmodel = keras.models.Sequential(\n    [\n        keras.layers.Embedding(max_features + 1, 128, input_length=maxlen),\n        keras.layers.Bidirectional(\n            keras.layers.LSTM(\n                64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True\n            )\n        ),\n        keras.layers.Bidirectional(\n            keras.layers.LSTM(\n                64,\n                dropout=0.2,\n                recurrent_dropout=0.2,\n            )\n        ),\n        keras.layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\n\n\nmodel.summary()\n\nMorphVocab()\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 300, 128)          2560128   \n                                                                 \n bidirectional (Bidirectiona  (None, 300, 128)         98816     \n l)                                                              \n                                                                 \n bidirectional_1 (Bidirectio  (None, 128)              98816     \n nal)                                                            \n                                                                 \n dense (Dense)               (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 2,757,889\nTrainable params: 2,757,889\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# import keras_tuner\n\n\ncallback = keras.callbacks.EarlyStopping()\n\n\nmodel.compile(keras.optimizers.Adam(0.01), \"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(\n    X_train,\n    y_train,\n    batch_size=32,\n    epochs=2,\n    validation_data=(X_test, y_test),\n    callbacks=[callback],\n)\n\nlen(history.history[\"loss\"])\n\nEpoch 1/2\n154/242 [==================&gt;...........] - ETA: 5:35 - loss: 0.4757 - accuracy: 0.7695\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nd:\\My_Projects\\blog\\blog\\interactive\\bLSTM-toxicity-classifier-RU.ipynb Cell 12 line 2\n      &lt;a href='vscode-notebook-cell:/d%3A/My_Projects/blog/blog/interactive/bLSTM-toxicity-classifier-RU.ipynb#X14sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; model.compile(keras.optimizers.Adam(0.01), \"binary_crossentropy\", metrics=[\"accuracy\"])\n----&gt; &lt;a href='vscode-notebook-cell:/d%3A/My_Projects/blog/blog/interactive/bLSTM-toxicity-classifier-RU.ipynb#X14sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; history = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test), callbacks=[callback])\n      &lt;a href='vscode-notebook-cell:/d%3A/My_Projects/blog/blog/interactive/bLSTM-toxicity-classifier-RU.ipynb#X14sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt; len(history.history['loss'])\n\nFile d:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n     63 filtered_tb = None\n     64 try:\n---&gt; 65     return fn(*args, **kwargs)\n     66 except Exception as e:\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n\nFile d:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\keras\\engine\\training.py:1564, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n   1556 with tf.profiler.experimental.Trace(\n   1557     \"train\",\n   1558     epoch_num=epoch,\n   (...)\n   1561     _r=1,\n   1562 ):\n   1563     callbacks.on_train_batch_begin(step)\n-&gt; 1564     tmp_logs = self.train_function(iterator)\n   1565     if data_handler.should_sync:\n   1566         context.async_wait()\n\nFile d:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    148 filtered_tb = None\n    149 try:\n--&gt; 150   return fn(*args, **kwargs)\n    151 except Exception as e:\n    152   filtered_tb = _process_traceback_frames(e.__traceback__)\n\nFile d:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915, in Function.__call__(self, *args, **kwds)\n    912 compiler = \"xla\" if self._jit_compile else \"nonXla\"\n    914 with OptionalXlaContext(self._jit_compile):\n--&gt; 915   result = self._call(*args, **kwds)\n    917 new_tracing_count = self.experimental_get_tracing_count()\n    918 without_tracing = (tracing_count == new_tracing_count)\n\nFile d:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947, in Function._call(self, *args, **kwds)\n    944   self._lock.release()\n    945   # In this case we have created variables on the first call, so we run the\n    946   # defunned version which is guaranteed to never create variables.\n--&gt; 947   return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n    948 elif self._stateful_fn is not None:\n    949   # Release the lock early so that multiple threads can perform the call\n    950   # in parallel.\n    951   self._lock.release()\n\nFile d:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496, in Function.__call__(self, *args, **kwargs)\n   2493 with self._lock:\n   2494   (graph_function,\n   2495    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n-&gt; 2496 return graph_function._call_flat(\n   2497     filtered_flat_args, captured_inputs=graph_function.captured_inputs)\n\nFile d:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862, in ConcreteFunction._call_flat(self, args, captured_inputs, cancellation_manager)\n   1858 possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n   1859 if (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n   1860     and executing_eagerly):\n   1861   # No tape is watching; skip to running the function.\n-&gt; 1862   return self._build_call_outputs(self._inference_function.call(\n   1863       ctx, args, cancellation_manager=cancellation_manager))\n   1864 forward_backward = self._select_forward_and_backward_functions(\n   1865     args,\n   1866     possible_gradient_type,\n   1867     executing_eagerly)\n   1868 forward_function, args_with_tangents = forward_backward.forward()\n\nFile d:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499, in _EagerDefinedFunction.call(self, ctx, args, cancellation_manager)\n    497 with _InterpolateFunctionError(self):\n    498   if cancellation_manager is None:\n--&gt; 499     outputs = execute.execute(\n    500         str(self.signature.name),\n    501         num_outputs=self._num_outputs,\n    502         inputs=args,\n    503         attrs=attrs,\n    504         ctx=ctx)\n    505   else:\n    506     outputs = execute.execute_with_cancellation(\n    507         str(self.signature.name),\n    508         num_outputs=self._num_outputs,\n   (...)\n    511         ctx=ctx,\n    512         cancellation_manager=cancellation_manager)\n\nFile d:\\My_Projects\\blog\\blog\\interactive\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     52 try:\n     53   ctx.ensure_initialized()\n---&gt; 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n     55                                       inputs, attrs, num_outputs)\n     56 except core._NotOkStatusException as e:\n     57   if name is not None:\n\nKeyboardInterrupt: \n\n\n\n\nsequences = tokenizer.texts_to_sequences(\n    [\"Я хочу есть\", \"Я люблю тебя\", \"на работе был полный пиддес :| и так каждое за...\"]\n)\ntest = sequence.pad_sequences(sequences, maxlen=maxlen)\n\noutput = model.predict(test)\n\nprint(output.flatten())\n\n1/1 [==============================] - 1s 1s/step\n[0.5422895  0.98179567 0.01973787]\n\n\n\nprint(sequences)\n\n\nmodel.save(\"model.keras\")\n\n\nloaded_model = keras.models.load_model(\"model.keras\")\n\n\nsequences = tokenizer.texts_to_sequences(\n    [\"Я хочу есть\", \"Я люблю тебя\", \"на работе был полный пиддес :| и так каждое за...\"]\n)\ntest = sequence.pad_sequences(sequences, maxlen=maxlen)\n\noutput = loaded_model.predict(test)\n\nprint(output.flatten())\n\n1/1 [==============================] - 0s 98ms/step\n[0.5422895  0.98179567 0.01973787]"
  },
  {
    "objectID": "blog/Untitled-1.html",
    "href": "blog/Untitled-1.html",
    "title": "draft: true",
    "section": "",
    "text": "draft: true\n\n1 + 1\n\n2"
  },
  {
    "objectID": "certificates.html",
    "href": "certificates.html",
    "title": "Certificates",
    "section": "",
    "text": "Scientific Computing with Python \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        Data Analysis with Python \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        Machine Learning with Python \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        Analyzing Social Media Data in Python \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        Writing Efficient R Code \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        Intermediate Google Sheets \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        Exploratory Data Analysis in R \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        Intermediate R \n      \n    \n    \n      \n    \n    \n  \n  \n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Sea Level Predictor\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMedical Data Visualizer\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSpam Classifier Model\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPage View Time Series Visualizer\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Costs Predictor Model\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nToxicity Classifier in Russian\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBook Recommendation Model\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Projects & Software",
    "section": "",
    "text": "Neural Network from Scratch in Cangjie \n      \n    \n    \n      \n    \n    \n    \n      Recreation of an artificial neural network described in the book Neural Networks from Scratch by Harrison Kinsley in Cangjie instead of Python.\n      \n    \n    \n  \n  \n  \n    \n      \n        Numcj - Numpy alternative for Cangjie \n      \n    \n    \n      \n    \n    \n    \n      基于matrix4cj numcj重新创建numpy中的函数和API\n      \n    \n    \n  \n  \n  \n    \n      \n        Vaccination Goal Visualizer \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        DistroWatch Data Explorer \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        Python OS \n        \n          \n        \n        \n      \n    \n    \n      \n    \n    \n    \n      This project is a spiritual successor to Snakeware, an attempt to create an OS similar to Commodore KERNAL but running Python instead of BASIC programming language. Runs TUI written with Textual inspired by Sevastolink terminals in the game Alien Isolation and other games such as Atomic Heart.\n      \n    \n    \n  \n  \n  \n    \n      \n        Book Recommendation Model \n      \n    \n    \n      \n    \n    \n  \n  \n  \n    \n      \n        ADS-B Tracker \n      \n    \n    \n      \n    \n    \n    \n      Simple flight tracker and weather radar\n      \n    \n    \n  \n  \n  \n    \n      \n        PDF Summarizer & Renamer \n      \n    \n    \n      \n    \n    \n    \n      Briefly summarizes PDFs in German and English and renames the files accordingly. Uses 3 different models from fastest and least accurate to slowest and most accurate (T5 transformer); can handle individual files as well as batch-process multiple documents.\n      \n    \n    \n  \n  \n  \n    \n      \n        MarktGuru Scraper \n      \n    \n    \n      \n    \n    \n    \n      Native app written in Python and web GUI that searches for lowest price products in the shopping list in German supermarkets and prepares a detailed report.Main features:- ZIP code selection- System to filter out unwanted results- Blacklist function to assist the filter- Excel export- The ability to mark lowest price either by the item searched or by product name- Responsive UI\n      \n    \n    \n  \n  \n  \n    \n      \n        Custom document management system \n      \n    \n    \n      \n    \n    \n    \n      OCR application with the ability to identify and repair damaged and encrypted files -- tested on thousands of archived documents.Main features:- Tesseract engine- OCR settings panel- Blockchain that keeps track of file changes- Log panel for debugging- CRON-like scheduling- Ability to mark files for redoing OCR with another method on the next run- Desktop+Mobile UI\n      \n    \n    \n  \n  \n  \n    \n      \n        File Copier \n      \n    \n    \n      \n    \n    \n    \n      Native app written in Python and web GUI to search and copy/move files from nested folders into a single folder; useful for cleaning old WeChat/QQ backups that may have important files that you want to save. I didn't find any program that could do that, so I wrote my own.\n      \n    \n    \n  \n  \n  \n    \n      \n        Drawing for Two \n      \n    \n    \n      \n    \n    \n    \n      A shared drawing app for two... maybe even three. Made with Websockets and a custom Flask server.\n      \n      \n      \n      \n        Demo:\n        https://www.bilibili.com/video/BV1a9FKecEvN/\n      \n      \n    \n    \n  \n  \n  \n    \n      \n        Peer-to-peer Conference App \n      \n    \n    \n      \n    \n    \n    \n      An installable multi peer-to-peer conference PWA that includes video chat capability, screen-sharing on desktop, sending messages and images. Made with Vue, WebRTC and a custom STUN server.\n      \n    \n    \n  \n  \n  \n    \n      \n        Rock Paper Scissors Lizard Spock Game \n      \n    \n    \n      \n    \n    \n    \n      Play against the computer and see your highscore.\n      \n    \n    \n  \n  \n  \n    \n      \n        APIs \n      \n    \n    \n      \n    \n    \n    \n      Collection of data endpoints for my projects. Uses FastAPI with OpenAPI specification and PostgreSQL for data caching.\n      \n    \n    \n  \n  \n\n\nNo matching items"
  }
]